{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-19T20:53:59.578799Z",
     "start_time": "2025-04-19T20:53:59.576010Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as  plt\n",
    "import seaborn as sns\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T20:54:02.214284Z",
     "start_time": "2025-04-19T20:54:02.142732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train= pd.read_csv(\"data/preprocessed/X_train.csv\")\n",
    "X_test = pd.read_csv(\"data/preprocessed/X_test.csv\")\n",
    "y_train = pd.read_csv(\"data/preprocessed/y_train.csv\")\n",
    "y_test = pd.read_csv(\"data/preprocessed/y_test.csv\")"
   ],
   "id": "7e97b2aba1842bb",
   "outputs": [],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T20:54:02.993006Z",
     "start_time": "2025-04-19T20:54:02.990174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ],
   "id": "ab1cdd8db451bbfd",
   "outputs": [],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T20:54:03.604388Z",
     "start_time": "2025-04-19T20:54:03.600942Z"
    }
   },
   "cell_type": "code",
   "source": "X_train.shape, y_train.shape, X_test.shape, y_test.shape",
   "id": "5ba9c00349335035",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80000, 9), (80000, 2), (20000, 9), (20000, 2))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T20:54:04.366046Z",
     "start_time": "2025-04-19T20:54:04.360387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BalancedEmailClickModel(nn.Module):\n",
    "    def __init__(self, input_dims=9, hidden_dims=(32, 16), dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Simpler shared network with balanced capacity\n",
    "        self.shared_network = nn.Sequential(\n",
    "            nn.Linear(input_dims, hidden_dims[0]),\n",
    "            nn.BatchNorm1d(hidden_dims[0]),\n",
    "            nn.LeakyReLU(0.1),  # LeakyReLU to prevent dying neurons\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.BatchNorm1d(hidden_dims[1]),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        # Simple head for open prediction\n",
    "        self.opened_head = nn.Linear(hidden_dims[1], 1)\n",
    "\n",
    "        # Simple head for click prediction that takes opening probability into account\n",
    "        self.clicked_head = nn.Linear(hidden_dims[1] + 1, 1)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)  # Xavier for better initialization\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x, y_opened=None):\n",
    "        h = self.shared_network(x)\n",
    "        opened_logits = self.opened_head(h)\n",
    "\n",
    "        # Conditional input for clicked head\n",
    "        if y_opened is not None:\n",
    "            # During training when we have ground truth opened labels\n",
    "            cond = y_opened\n",
    "        else:\n",
    "            # During inference or validation\n",
    "            cond = torch.sigmoid(opened_logits).detach()\n",
    "\n",
    "        clicked_logits = self.clicked_head(torch.cat([h, cond], dim=1))\n",
    "\n",
    "        return opened_logits, clicked_logits\n",
    "\n",
    "    def loss(self, opened_logits, clicked_logits, y_opened, y_clicked, pos_weight=None):\n",
    "        \"\"\"Calculate balanced loss\"\"\"\n",
    "        # Basic losses with class balancing\n",
    "        loss_open = F.binary_cross_entropy_with_logits(opened_logits, y_opened)\n",
    "\n",
    "        # Handle click prediction with pos_weight (class imbalance)\n",
    "        if pos_weight is not None:\n",
    "            loss_click = F.binary_cross_entropy_with_logits(\n",
    "                clicked_logits, y_clicked, pos_weight=pos_weight)\n",
    "        else:\n",
    "            loss_click = F.binary_cross_entropy_with_logits(clicked_logits, y_clicked)\n",
    "\n",
    "        # Combined loss with fixed weighting\n",
    "        total_loss = 0.4 * loss_open + 0.6 * loss_click\n",
    "\n",
    "        return loss_open, loss_click, total_loss"
   ],
   "id": "82c49823e571a84d",
   "outputs": [],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T20:54:05.350359Z",
     "start_time": "2025-04-19T20:54:05.347491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, accuracy_score, f1_score\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import random_split\n",
    "# from tqdm.notebook import tqdm"
   ],
   "id": "3db52497ce7a3178",
   "outputs": [],
   "execution_count": 150
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T20:54:05.860562Z",
     "start_time": "2025-04-19T20:54:05.856301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, X, opened, clicked):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y_opened = torch.tensor(opened.values, dtype=torch.float32).unsqueeze(1)\n",
    "        self.y_clicked = torch.tensor(clicked.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], {\"opened\": self.y_opened[idx], \"clicked\": self.y_clicked[idx]}"
   ],
   "id": "55a4d5208a58cf5d",
   "outputs": [],
   "execution_count": 151
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T20:54:06.476329Z",
     "start_time": "2025-04-19T20:54:06.472726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(y_true, y_score, threshold=0.5):\n",
    "    \"\"\"Compute classification metrics given scores and labels\"\"\"\n",
    "    y_pred = [1.0 if p > threshold else 0.0 for p in y_score]\n",
    "\n",
    "    metrics = {\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "\n",
    "    # Only compute AUC if we have both positive and negative samples\n",
    "    if len(set(y_true)) > 1:\n",
    "        metrics['auc'] = roc_auc_score(y_true, y_score)\n",
    "    else:\n",
    "        metrics['auc'] = float('nan')\n",
    "\n",
    "    return metrics"
   ],
   "id": "260199a35b286ace",
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T20:54:06.980358Z",
     "start_time": "2025-04-19T20:54:06.977157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_train_val_datasets(dataset, val_ratio=0.2, seed=42):\n",
    "    total_size = len(dataset)\n",
    "    val_size = int(total_size * val_ratio)\n",
    "    train_size = total_size - val_size\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    return train_ds, val_ds"
   ],
   "id": "a917cdc10fe4495",
   "outputs": [],
   "execution_count": 153
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T20:54:07.549109Z",
     "start_time": "2025-04-19T20:54:07.542087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds = EmailDataset(X_train, y_train[\"opened\"], y_train[\"clicked\"])\n",
    "train_ds, val_ds = create_train_val_datasets(ds, val_ratio=0.2)"
   ],
   "id": "a175287b714d38fd",
   "outputs": [],
   "execution_count": 154
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T20:54:08.278819Z",
     "start_time": "2025-04-19T20:54:08.274375Z"
    }
   },
   "cell_type": "code",
   "source": "ds[0]",
   "id": "857fa907c8babe89",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.9659,  0.0000, -1.8102,  0.2588,  0.0000,  1.0000,  0.0000,  0.0000,\n",
       "          0.0000]),\n",
       " {'opened': tensor([0.]), 'clicked': tensor([0.])})"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 155
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T20:54:11.792912Z",
     "start_time": "2025-04-19T20:54:11.775567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_ds, val_ds,\n",
    "                epochs=50, batch_size=64, lr=1e-3,\n",
    "                warmup_epochs=5, patience=10,\n",
    "                class_weight_factor=2.0):\n",
    "    \"\"\"\n",
    "    Fixed training function that addresses overfitting issues:\n",
    "    - Simplified learning rate schedule\n",
    "    - Better handling of class imbalance\n",
    "    - Balanced batch sampling\n",
    "    - Better monitoring of metrics\n",
    "    - Lower learning rate\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Data loaders\n",
    "    # Compute class weights for balanced batches\n",
    "    loader_tr = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    loader_val = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "    # Compute positive class weight for handling imbalance - with a safeguard against extreme values\n",
    "    pos, neg = 0, 0\n",
    "    for _, y in loader_tr:\n",
    "        pos += y['clicked'].sum().item()\n",
    "        neg += (y['clicked'].size(0) - y['clicked'].sum().item())\n",
    "\n",
    "    # Limit the pos_weight to avoid extreme values that can destabilize training\n",
    "    raw_weight = neg / max(pos, 1)\n",
    "    pos_weight = torch.tensor(min(raw_weight, class_weight_factor * 10), device=device)\n",
    "    print(f\"Class imbalance ratio (neg/pos): {raw_weight:.2f}, using pos_weight: {pos_weight.item():.2f}\")\n",
    "\n",
    "    # Simple optimizer with reduced learning rate to prevent overfitting\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    # Simple step scheduler with warm restarts to avoid getting stuck\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "\n",
    "    # History tracking for both training and validation\n",
    "    history = {\n",
    "        'train': {k: [] for k in ['loss', 'opened_loss', 'clicked_loss',\n",
    "                                  'precision', 'recall', 'f1', 'opened_accuracy',\n",
    "                                  'clicked_accuracy', 'auc']},\n",
    "        'val': {k: [] for k in ['loss', 'opened_loss', 'clicked_loss',\n",
    "                               'precision', 'recall', 'f1', 'opened_accuracy',\n",
    "                               'clicked_accuracy', 'auc']}\n",
    "    }\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_val_f1 = -float('inf')  # Track F1 score instead of just loss\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_opened_losses = []\n",
    "        train_clicked_losses = []\n",
    "        train_true_opened, train_pred_opened = [], []\n",
    "        train_true_clicked, train_score_clicked = [], []\n",
    "\n",
    "        for X, y in loader_tr:\n",
    "            X = X.to(device)\n",
    "            y_open = y['opened'].to(device)\n",
    "            y_click = y['clicked'].to(device)\n",
    "\n",
    "            # Teacher forcing during warmup\n",
    "            is_warm = epoch <= warmup_epochs\n",
    "            opened_logits, clicked_logits = model(X, y_open if is_warm else None)\n",
    "\n",
    "            # Compute losses\n",
    "            l_open, l_click, loss = model.loss(\n",
    "                opened_logits, clicked_logits, y_open, y_click, pos_weight\n",
    "            )\n",
    "\n",
    "            # Backpropagation with gradient clipping\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track metrics\n",
    "            train_losses.append(loss.item())\n",
    "            train_opened_losses.append(l_open.item())\n",
    "            train_clicked_losses.append(l_click.item())\n",
    "\n",
    "            # Collect predictions for opened emails\n",
    "            train_true_opened.extend(y_open.cpu().numpy().flatten().tolist())\n",
    "            train_pred_opened.extend((torch.sigmoid(opened_logits) > 0.5).float().cpu().numpy().flatten().tolist())\n",
    "\n",
    "            # Collect predictions for clicked emails\n",
    "            train_true_clicked.extend(y_click.cpu().numpy().flatten().tolist())\n",
    "            train_score_clicked.extend(torch.sigmoid(clicked_logits).detach().cpu().numpy().flatten().tolist())\n",
    "\n",
    "        # Calculate training metrics\n",
    "        train_opened_acc = sum(1 for a, b in zip(train_true_opened, train_pred_opened) if a == b) / len(train_true_opened)\n",
    "        train_clicked_metrics = compute_metrics(train_true_clicked, train_score_clicked)\n",
    "        train_clicked_acc = sum(1 for a, b in zip(train_true_clicked, [1 if s > 0.5 else 0 for s in train_score_clicked])) / len(train_true_clicked)\n",
    "\n",
    "        # Update training history\n",
    "        history['train']['loss'].append(sum(train_losses) / len(train_losses))\n",
    "        history['train']['opened_loss'].append(sum(train_opened_losses) / len(train_opened_losses))\n",
    "        history['train']['clicked_loss'].append(sum(train_clicked_losses) / len(train_clicked_losses))\n",
    "        history['train']['precision'].append(train_clicked_metrics['precision'])\n",
    "        history['train']['recall'].append(train_clicked_metrics['recall'])\n",
    "        history['train']['f1'].append(train_clicked_metrics['f1'])\n",
    "        history['train']['auc'].append(train_clicked_metrics['auc'])\n",
    "        history['train']['opened_accuracy'].append(train_opened_acc)\n",
    "        history['train']['clicked_accuracy'].append(train_clicked_acc)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_opened_losses = []\n",
    "        val_clicked_losses = []\n",
    "        val_true_opened, val_pred_opened = [], []\n",
    "        val_true_clicked, val_score_clicked = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in loader_val:\n",
    "                X = X.to(device)\n",
    "                y_open = y['opened'].to(device)\n",
    "                y_click = y['clicked'].to(device)\n",
    "\n",
    "                # Forward pass - no teacher forcing during validation\n",
    "                opened_logits, clicked_logits = model(X)\n",
    "\n",
    "                # Calculate losses\n",
    "                l_open, l_click, loss = model.loss(\n",
    "                    opened_logits, clicked_logits, y_open, y_click, pos_weight\n",
    "                )\n",
    "\n",
    "                # Track metrics\n",
    "                val_losses.append(loss.item())\n",
    "                val_opened_losses.append(l_open.item())\n",
    "                val_clicked_losses.append(l_click.item())\n",
    "\n",
    "                # Collect predictions for opened emails\n",
    "                val_true_opened.extend(y_open.cpu().numpy().flatten().tolist())\n",
    "                val_pred_opened.extend((torch.sigmoid(opened_logits) > 0.5).float().cpu().numpy().flatten().tolist())\n",
    "\n",
    "                # Collect predictions for clicked emails\n",
    "                val_true_clicked.extend(y_click.cpu().numpy().flatten().tolist())\n",
    "                val_score_clicked.extend(torch.sigmoid(clicked_logits).cpu().numpy().flatten().tolist())\n",
    "\n",
    "        # Calculate validation metrics\n",
    "        val_opened_acc = sum(1 for a, b in zip(val_true_opened, val_pred_opened) if a == b) / len(val_true_opened)\n",
    "        val_clicked_metrics = compute_metrics(val_true_clicked, val_score_clicked)\n",
    "        val_clicked_acc = sum(1 for a, b in zip(val_true_clicked, [1 if s > 0.5 else 0 for s in val_score_clicked])) / len(val_true_clicked)\n",
    "\n",
    "        # Update validation history\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        history['val']['loss'].append(avg_val_loss)\n",
    "        history['val']['opened_loss'].append(sum(val_opened_losses) / len(val_opened_losses))\n",
    "        history['val']['clicked_loss'].append(sum(val_clicked_losses) / len(val_clicked_losses))\n",
    "        history['val']['precision'].append(val_clicked_metrics['precision'])\n",
    "        history['val']['recall'].append(val_clicked_metrics['recall'])\n",
    "        history['val']['f1'].append(val_clicked_metrics['f1'])\n",
    "        history['val']['auc'].append(val_clicked_metrics['auc'])\n",
    "        history['val']['opened_accuracy'].append(val_opened_acc)\n",
    "        history['val']['clicked_accuracy'].append(val_clicked_acc)\n",
    "\n",
    "        # Update learning rate based on validation performance\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        print(f\"  Train | loss: {history['train']['loss'][-1]:.4f} | opened_acc: {train_opened_acc:.4f} | clicked_acc: {train_clicked_acc:.4f}\")\n",
    "        print(f\"        | precision: {train_clicked_metrics['precision']:.4f} | recall: {train_clicked_metrics['recall']:.4f} | f1: {train_clicked_metrics['f1']:.4f} | auc: {train_clicked_metrics['auc']:.4f}\")\n",
    "        print(f\"  Val   | loss: {avg_val_loss:.4f} | opened_acc: {val_opened_acc:.4f} | clicked_acc: {val_clicked_acc:.4f}\")\n",
    "        print(f\"        | precision: {val_clicked_metrics['precision']:.4f} | recall: {val_clicked_metrics['recall']:.4f} | f1: {val_clicked_metrics['f1']:.4f} | auc: {val_clicked_metrics['auc']:.4f}\")\n",
    "\n",
    "        # Check for NaN or error values in validation metrics\n",
    "        if (val_clicked_metrics['precision'] == 0 and val_clicked_metrics['recall'] == 0) or \\\n",
    "           torch.isnan(torch.tensor(avg_val_loss)):\n",
    "            print(\"Warning: Detected zero precision/recall or NaN loss, adjusting model...\")\n",
    "            # Reduce learning rate and continue\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= 0.5\n",
    "\n",
    "            # If we've had multiple issues, break early\n",
    "            if patience_counter > patience // 2:\n",
    "                print(\"Training unstable, breaking early\")\n",
    "                break\n",
    "\n",
    "        # Early stopping logic - based on F1 score, not just loss\n",
    "        val_f1 = val_clicked_metrics['f1']\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch} epochs\")\n",
    "                break\n",
    "\n",
    "    # Load best model if early stopping was triggered\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"Restored best model with F1 score: {best_val_f1:.4f}\")\n",
    "\n",
    "    return model, history"
   ],
   "id": "2c7de567f0fd7375",
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T20:54:12.913134Z",
     "start_time": "2025-04-19T20:54:12.909477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def visualize_metrics(history):\n",
    "    \"\"\"Visualize training and validation metrics\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    metrics = ['loss', 'precision', 'recall', 'f1', 'auc', 'opened_accuracy', 'clicked_accuracy']\n",
    "    fig, axes = plt.subplots(len(metrics), 1, figsize=(12, 4*len(metrics)))\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i]\n",
    "        ax.plot(history['train'][metric], label=f'Train {metric}')\n",
    "        ax.plot(history['val'][metric], label=f'Val {metric}')\n",
    "        ax.set_title(f'{metric.capitalize()} vs. Epoch')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel(metric.capitalize())\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ],
   "id": "6616e8f497afe041",
   "outputs": [],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T20:54:13.713270Z",
     "start_time": "2025-04-19T20:54:13.709494Z"
    }
   },
   "cell_type": "code",
   "source": "model = BalancedEmailClickModel()",
   "id": "e2d63b3077a55fc7",
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T20:54:16.830385Z",
     "start_time": "2025-04-19T20:54:16.824560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Implementation to find optimal threshold\n",
    "def find_optimal_threshold(model, val_loader, device):\n",
    "    \"\"\"Find optimal threshold for classification by maximizing F1 score\"\"\"\n",
    "    model.eval()\n",
    "    all_true = []\n",
    "    all_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X = X.to(device)\n",
    "            y_click = y['clicked'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            _, clicked_logits = model(X)\n",
    "\n",
    "            # Collect predictions\n",
    "            all_true.extend(y_click.cpu().numpy().flatten().tolist())\n",
    "            all_scores.extend(torch.sigmoid(clicked_logits).cpu().numpy().flatten().tolist())\n",
    "\n",
    "    # Try different thresholds\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "\n",
    "    for threshold in [i/100 for i in range(1, 100)]:\n",
    "        y_pred = [1 if score > threshold else 0 for score in all_scores]\n",
    "        f1 = f1_score(all_true, y_pred, zero_division=0)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    print(f\"Optimal threshold: {best_threshold:.2f} with F1 score: {best_f1:.4f}\")\n",
    "    return best_threshold"
   ],
   "id": "2303c2e806272885",
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T20:55:43.533608Z",
     "start_time": "2025-04-19T20:55:11.343523Z"
    }
   },
   "cell_type": "code",
   "source": "model, history = train_model(model, train_ds, val_ds, epochs=150, batch_size=128, lr=3e-4, warmup_epochs=50, patience=20)",
   "id": "2b7ed8148a8d1be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class imbalance ratio (neg/pos): 46.83, using pos_weight: 20.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kraten/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "  Train | loss: 0.5058 | opened_acc: 0.8946 | clicked_acc: 1.0000\n",
      "        | precision: 0.1488 | recall: 0.5247 | f1: 0.2318 | auc: 0.8564\n",
      "  Val   | loss: 0.5574 | opened_acc: 0.8986 | clicked_acc: 1.0000\n",
      "        | precision: 0.0687 | recall: 0.2012 | f1: 0.1024 | auc: 0.7506\n",
      "Epoch 2/150\n",
      "  Train | loss: 0.4920 | opened_acc: 0.8956 | clicked_acc: 1.0000\n",
      "        | precision: 0.1823 | recall: 0.5531 | f1: 0.2742 | auc: 0.8746\n",
      "  Val   | loss: 0.5560 | opened_acc: 0.8986 | clicked_acc: 1.0000\n",
      "        | precision: 0.0720 | recall: 0.1921 | f1: 0.1047 | auc: 0.7529\n",
      "Epoch 3/150\n",
      "  Train | loss: 0.4796 | opened_acc: 0.8955 | clicked_acc: 1.0000\n",
      "        | precision: 0.1940 | recall: 0.5987 | f1: 0.2930 | auc: 0.8886\n",
      "  Val   | loss: 0.5557 | opened_acc: 0.8986 | clicked_acc: 1.0000\n",
      "        | precision: 0.0758 | recall: 0.1738 | f1: 0.1056 | auc: 0.7539\n",
      "Epoch 4/150\n",
      "  Train | loss: 0.4669 | opened_acc: 0.8959 | clicked_acc: 1.0000\n",
      "        | precision: 0.2237 | recall: 0.6248 | f1: 0.3295 | auc: 0.9038\n",
      "  Val   | loss: 0.5565 | opened_acc: 0.8986 | clicked_acc: 1.0000\n",
      "        | precision: 0.0854 | recall: 0.1677 | f1: 0.1132 | auc: 0.7532\n",
      "Epoch 5/150\n",
      "  Train | loss: 0.4556 | opened_acc: 0.8960 | clicked_acc: 1.0000\n",
      "        | precision: 0.2367 | recall: 0.6465 | f1: 0.3465 | auc: 0.9159\n",
      "  Val   | loss: 0.5573 | opened_acc: 0.8986 | clicked_acc: 1.0000\n",
      "        | precision: 0.0927 | recall: 0.1463 | f1: 0.1135 | auc: 0.7536\n",
      "Epoch 6/150\n",
      "  Train | loss: 0.4450 | opened_acc: 0.8962 | clicked_acc: 1.0000\n",
      "        | precision: 0.2454 | recall: 0.6951 | f1: 0.3628 | auc: 0.9242\n",
      "  Val   | loss: 0.5592 | opened_acc: 0.8986 | clicked_acc: 1.0000\n",
      "        | precision: 0.1016 | recall: 0.0793 | f1: 0.0890 | auc: 0.7532\n",
      "Epoch 7/150\n",
      "  Train | loss: 0.4323 | opened_acc: 0.8963 | clicked_acc: 1.0000\n",
      "        | precision: 0.2508 | recall: 0.7235 | f1: 0.3725 | auc: 0.9355\n",
      "  Val   | loss: 0.5604 | opened_acc: 0.8986 | clicked_acc: 1.0000\n",
      "        | precision: 0.1221 | recall: 0.0488 | f1: 0.0697 | auc: 0.7562\n",
      "Epoch 8/150\n",
      "  Train | loss: 0.4240 | opened_acc: 0.8964 | clicked_acc: 1.0000\n",
      "        | precision: 0.2467 | recall: 0.7399 | f1: 0.3700 | auc: 0.9403\n",
      "  Val   | loss: 0.5611 | opened_acc: 0.8986 | clicked_acc: 1.0000\n",
      "        | precision: 0.1184 | recall: 0.0274 | f1: 0.0446 | auc: 0.7578\n",
      "Epoch 9/150\n",
      "  Train | loss: 0.4151 | opened_acc: 0.8964 | clicked_acc: 1.0000\n",
      "        | precision: 0.2448 | recall: 0.7534 | f1: 0.3695 | auc: 0.9452\n",
      "  Val   | loss: 0.5652 | opened_acc: 0.8986 | clicked_acc: 1.0000\n",
      "        | precision: 0.1786 | recall: 0.0152 | f1: 0.0281 | auc: 0.7583\n",
      "Epoch 10/150\n",
      "  Train | loss: 0.4081 | opened_acc: 0.8964 | clicked_acc: 1.0000\n",
      "        | precision: 0.2480 | recall: 0.7728 | f1: 0.3755 | auc: 0.9492\n",
      "  Val   | loss: 0.5657 | opened_acc: 0.8986 | clicked_acc: 1.0000\n",
      "        | precision: 0.1579 | recall: 0.0183 | f1: 0.0328 | auc: 0.7595\n",
      "Epoch 11/150\n",
      "  Train | loss: 0.4042 | opened_acc: 0.8964 | clicked_acc: 1.0000\n",
      "        | precision: 0.2442 | recall: 0.7765 | f1: 0.3716 | auc: 0.9495\n",
      "  Val   | loss: 0.5645 | opened_acc: 0.8986 | clicked_acc: 1.0000\n",
      "        | precision: 0.1471 | recall: 0.0152 | f1: 0.0276 | auc: 0.7601\n",
      "Epoch 12/150\n",
      "  Train | loss: 0.3999 | opened_acc: 0.8964 | clicked_acc: 1.0000\n",
      "        | precision: 0.2394 | recall: 0.7870 | f1: 0.3671 | auc: 0.9524\n",
      "  Val   | loss: 0.5694 | opened_acc: 0.8986 | clicked_acc: 1.0000\n",
      "        | precision: 0.1538 | recall: 0.0061 | f1: 0.0117 | auc: 0.7599\n",
      "Epoch 13/150\n",
      "  Train | loss: 0.3955 | opened_acc: 0.8964 | clicked_acc: 1.0000\n",
      "        | precision: 0.2434 | recall: 0.8064 | f1: 0.3739 | auc: 0.9534\n",
      "  Val   | loss: 0.5693 | opened_acc: 0.8986 | clicked_acc: 1.0000\n",
      "        | precision: 0.0909 | recall: 0.0030 | f1: 0.0059 | auc: 0.7598\n",
      "Epoch 14/150\n",
      "  Train | loss: 0.3927 | opened_acc: 0.8964 | clicked_acc: 1.0000\n",
      "        | precision: 0.2372 | recall: 0.8019 | f1: 0.3661 | auc: 0.9538\n",
      "  Val   | loss: 0.5693 | opened_acc: 0.8986 | clicked_acc: 1.0000\n",
      "        | precision: 0.2000 | recall: 0.0061 | f1: 0.0118 | auc: 0.7591\n",
      "Epoch 15/150\n",
      "  Train | loss: 0.3901 | opened_acc: 0.8964 | clicked_acc: 1.0000\n",
      "        | precision: 0.2355 | recall: 0.8124 | f1: 0.3651 | auc: 0.9540\n",
      "  Val   | loss: 0.5674 | opened_acc: 0.8986 | clicked_acc: 1.0000\n",
      "        | precision: 0.1667 | recall: 0.0061 | f1: 0.0118 | auc: 0.7612\n",
      "Epoch 16/150\n",
      "  Train | loss: 0.3875 | opened_acc: 0.8964 | clicked_acc: 1.0000\n",
      "        | precision: 0.2354 | recall: 0.8236 | f1: 0.3661 | auc: 0.9542\n",
      "  Val   | loss: 0.5717 | opened_acc: 0.8986 | clicked_acc: 1.0000\n",
      "        | precision: 0.2500 | recall: 0.0061 | f1: 0.0119 | auc: 0.7604\n",
      "Epoch 17/150\n",
      "  Train | loss: 0.3839 | opened_acc: 0.8964 | clicked_acc: 1.0000\n",
      "        | precision: 0.2372 | recall: 0.8356 | f1: 0.3695 | auc: 0.9558\n",
      "  Val   | loss: 0.5708 | opened_acc: 0.8986 | clicked_acc: 1.0000\n",
      "        | precision: 0.1667 | recall: 0.0030 | f1: 0.0060 | auc: 0.7608\n",
      "Epoch 18/150\n",
      "  Train | loss: 0.3838 | opened_acc: 0.8964 | clicked_acc: 1.0000\n",
      "        | precision: 0.2331 | recall: 0.8274 | f1: 0.3637 | auc: 0.9557\n",
      "  Val   | loss: 0.5711 | opened_acc: 0.8986 | clicked_acc: 1.0000\n",
      "        | precision: 0.1429 | recall: 0.0030 | f1: 0.0060 | auc: 0.7614\n",
      "Epoch 19/150\n",
      "  Train | loss: 0.3814 | opened_acc: 0.8964 | clicked_acc: 1.0000\n",
      "        | precision: 0.2321 | recall: 0.8296 | f1: 0.3627 | auc: 0.9564\n",
      "  Val   | loss: 0.5744 | opened_acc: 0.8986 | clicked_acc: 1.0000\n",
      "        | precision: 0.0000 | recall: 0.0000 | f1: 0.0000 | auc: 0.7610\n",
      "Warning: Detected zero precision/recall or NaN loss, adjusting model...\n",
      "Training unstable, breaking early\n",
      "Restored best model with F1 score: 0.1135\n"
     ]
    }
   ],
   "execution_count": 162
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bcdb255acd50b4e2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
